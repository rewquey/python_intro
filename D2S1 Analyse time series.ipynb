{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetition from Day 1\n",
    "ToDo: Which topics to repeat? Prepare here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2, before lunch: Time series analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: Read stuff using pandas, making use of it's time series analysis and statistics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table(\n",
    "    'd1s2/rr24_Bulken.txt', encoding='utf8', \n",
    "    header=13,\n",
    "    skipfooter=12, engine='python',  # The file contains 12 footer lines, and skipfooter requires the python parsing engine\n",
    "    sep='\\s+',                       # Column separtion by one or more whitespace\n",
    "    parse_dates=[1,], dayfirst=True, # Second column contains dates, in European format\n",
    "    index_col=1,                     # Use the date column as index\n",
    ")\n",
    "\n",
    "# Note the use of column headers as attributes!\n",
    "# df.RR.plot()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this ``df.RR``, and what else can we do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(df.RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many things available! We'll dive into a few of them later. \n",
    "\n",
    "First, a quick and easy first data analysis: Cumulative precipitation during that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.RR.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: ``cumsum()`` returns a time series, so we can work with that in exactly the same way as ``df.RR``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.RR.cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second example, find dates where the 24-hour precipitation exceeded 50 mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[df.RR > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.RR > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth taking some time to figure out in detail what happens here.\n",
    "\n",
    "Todo: Give some more details also here in text. Explain the above df.loc, df.iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.RR > 50), (df.RR > 50).dtype, len(df.RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a boolean time series to select dates.\n",
    "\n",
    "We could also use any other (random) boolean time series of length 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "randombools = pd.Series(np.random.rand(100) >= 0.9, index=df.RR)\n",
    "randombools.sum() # How many True values (=ones) are there in the time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[randombools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing time series\n",
    "\n",
    "So far we have only worked with one time series. Let's add a second to have some more analysis options to explore.\n",
    "\n",
    "Unfortunately, the date format of ``rro_Bulken.txt`` is not recognised automatically by pandas, so we need to supply our custom conversion function.\n",
    "\n",
    "ToDo: Explain the ``lambda``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date_parser = lambda datestr: datetime.strptime(datestr, '%d%m%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``lambda`` is essentially just a shorthand for defining a function. We can use ``date_parser`` just as any other function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_parser('23012019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_table('d1s2/rro_Bulken.txt', encoding='latin1', \n",
    "                   header=None, names=['Dato', 'Level', 'Discharge', 'p75', 'p50', 'p25'],\n",
    "                                                    # Custom header information\n",
    "                   comment='#',                     # Ignore lines starting with #\n",
    "                   na_values=['----', ],            # Custom marker for missing values\n",
    "                   sep='\\s+',                       # Column separtion by one or more whitespace\n",
    "                   parse_dates=[0,], date_parser=date_parser, \n",
    "                                                    # First column contains dates, custom format\n",
    "                   index_col=0,                     # Use the date column as index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check to see whether we got what we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Discharge.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analyses\n",
    "\n",
    "Is river runoff correlated to precipitation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the different time periods for df and df2!\n",
    "df.RR.corr(df2.Discharge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like!\n",
    "\n",
    "But what about the median discharge and precipitation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.p50.corr(df.RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advance statistics will require the ``scipy.stats`` module. But to be able to use that module, we'll need to homogenise the two time series:\n",
    " * Identical index, i.e. dates\n",
    " * Remove NaNs\n",
    " \n",
    "Once complete, we'll then make use of ``scipy.stats`` to estimate the significance of the above correlations and calculate lagged correlations between the two time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First combine into common data frame, using the same index\n",
    "dfa = pd.concat([df, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then extract the columns we are interested in, keeping only those rows where we have data in both\n",
    "df_clean = dfa[['RR', 'Discharge']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're finally ready to calculate the Pearson correlaton including it's significance. Documentation for the function is given here:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#scipy.stats.pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "scipy.stats.pearsonr(df_clean.RR, df_clean.Discharge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, seems highly significant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = dfa[['RR', 'p50']].dropna()\n",
    "scipy.stats.pearsonr(df_clean.RR, df_clean.p50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the correlation between median discharge and precipitation seems spurious.\n",
    "\n",
    "With that clarified, onto lagged correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Lagged correlations\n",
    "\n",
    "At which lead/lag in days does the correlation between precipitation reach it's maximum?\n",
    "\n",
    "The ``pandas.Series.shift`` function might come in handy for the analysis.\n",
    "\n",
    "Todo: Break up into several steps, smaller assignment pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tshifts = np.arange(-5,6)\n",
    "corrcoefs = np.empty(tshifts.shape)\n",
    "pvalues = np.empty(tshifts.shape)\n",
    "for i, tshift in zip(range(len(tshifts)), tshifts):\n",
    "    RRshift = df.RR.shift(tshift)\n",
    "    dfs = pd.concat([RRshift, df2.Discharge], axis=1)\n",
    "    dfs = dfs.dropna()\n",
    "    corrcoefs[i], pvalues[i] = scipy.stats.pearsonr(dfs.RR, dfs.Discharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tshifts, corrcoefs, linewidth=1, color='C0')\n",
    "plt.plot(tshifts[pvalues < 0.01], corrcoefs[pvalues < 0.01], linewidth=3, color='C0')\n",
    "plt.xlabel('<- Discharge leads / Rain leads -> [days]')\n",
    "plt.ylabel('Correlation coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Smoothing time series\n",
    "\n",
    "The river discharge might contain an integral of the precipitation over the preceeding days. Let's correlate smoothed precipitation with discharge, to see whether we can further increase the correlations. Use running means of 1-5 days centred on the given date in combination with lags between -5 and 5 days to find the maximum correlation.\n",
    "\n",
    "The ``pandas.Series.rolling`` function might come in handy for calculating the running mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmeans = np.arange(1,6)\n",
    "tshifts = np.arange(-5,6)\n",
    "corrcoefs = np.empty(rmeans.shape+tshifts.shape)\n",
    "pvalues = np.empty(rmeans.shape+tshifts.shape)\n",
    "for j, rmean in zip(range(len(rmeans)), rmeans):\n",
    "    RRmean = df.RR.rolling(window=rmean, center=True).mean()\n",
    "    for i, tshift in zip(range(len(tshifts)), tshifts):\n",
    "        RRshift = RRmean.shift(tshift)\n",
    "        dfs = pd.concat([RRshift, df2.Discharge], axis=1)\n",
    "        dfs = dfs.dropna()\n",
    "        corrcoefs[j,i], pvalues[j,i] = scipy.stats.pearsonr(dfs.RR, dfs.Discharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, rmean in zip(range(len(rmeans)), rmeans):\n",
    "    plt.plot(tshifts, corrcoefs[j], linewidth=1, color='C%d' % j)\n",
    "    plt.plot(tshifts[pvalues[j,:] < 0.01], corrcoefs[j,pvalues[j,:] < 0.01], \n",
    "             linewidth=3, color='C%d' % j, label='%d-day mean' % rmean)\n",
    "plt.xlabel('<- Discharge leads / Rain leads -> [days]')\n",
    "plt.ylabel('Correlation coefficient')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fitting linear model\n",
    "\n",
    "For the combination of running mean and time lag that yields the maximum correlation create a linear model to estimate discharge from observed precipitation. Visualise the results in a scatter as well as a time-series plot comparing predicted and actual discharge.\n",
    "\n",
    "ToDo: Break up into pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the following, make it NOT part of the assignment.\n",
    "jmax, imax = np.unravel_index(np.argmax*(corrcoefs), corrcoefs.shape)\n",
    "print('Maximum correlation of %3.1f%% at rmean %d days and lag %d days' % (corrcoefs.max()*100, rmeans[jmax], tshifts[imax]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the shifted running mean that gave the maximum correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RRmax = df.RR.rolling(window=rmeans[jmax], center=True).mean().shift(tshifts[imax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all relevant ``Series`` in a new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column to avoid having two columns named \"RR\", then concatenate to one dataframe\n",
    "RRmax.rename('RRmax', inplace=True)\n",
    "dfs = pd.concat([RRmax, df.RR, df2.Discharge], axis=1)\n",
    "dfs_clean = dfs.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the linear model by linear regression\n",
    "\n",
    "Todo: Here the actual assignment starts, give hints ``scipy.stats.linregress``. May be make a standard function from the labmda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel = scipy.stats.linregress(dfs_clean.RRmax, dfs_clean.Discharge)\n",
    "estimate_Discharge = lambda RRm: linmodel.slope*RRm + linmodel.intercept\n",
    "dfs['Discharge_linmodel'] = pd.Series(estimate_Discharge(dfs.RRmax), index=dfs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot for evaluation\n",
    "\n",
    "Todo: Hint ``scatter``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dfs.Discharge, dfs.Discharge_linmodel)\n",
    "plt.plot([0, 500], [0, 500], 'k-', linewidth=1)\n",
    "plt.xlabel('Observed discharge [m3/s]')\n",
    "plt.ylabel('Estimated discharge from precipitation [m3/s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series plot for evaluation\n",
    "\n",
    "Hint: use ``label = 'String'`` as a keyword argument to the plot. Doublecheck whether legend is new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_clean = dfs.dropna()\n",
    "dfs_clean.Discharge.plot(label='Observed')\n",
    "dfs_clean.Discharge_linmodel.plot(label='Estimated')\n",
    "plt.ylabel('Discharge [m3/s]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "83e4dae74d7a452da845a83c66caccd5",
   "lastKernelId": "64f512d6-5e02-4551-b6f0-243f0c156747"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
