{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetition from days 1+2\n",
    "\n",
    "We went through\n",
    " * Basic usage of python. Operators, data structures, flow control.\n",
    " * Manually and partly-automatic parsing of text files.\n",
    " * Some basic and not-so-basic plotting: double y-axes, logarithmic y axis, line graphs, bars and filled areas.\n",
    "\n",
    "Questions\n",
    " * Anything that was (particularly) unclear?\n",
    " * Follow-up questions to what we went through?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3, before lunch: Time series analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: Read stuff using pandas, making use of it's time series analysis and statistics functions.\n",
    "\n",
    "But before we can start with that, we need to understand a little the basic data structure in pandas, a ``DataFrame``. Very briefly a ``DataFrame`` is a bit like a Table in an Excel/Calc spreadsheat:\n",
    "\n",
    " * It has columns and rows (hence two-dimensional structure).\n",
    " * Both columns and rows can have a header (a name).\n",
    " * ``DataFrames`` also have an index, a unique identifier for each entry (i.e. row) in the table.\n",
    " * Columns can have different data types\n",
    "\n",
    "Let's see how we can build a ``DataFrame`` from the structures we already know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfa = pd.DataFrame([3, 6, 9, 15, 24])\n",
    "dfb = pd.DataFrame([(1,3), (1,6), (2,9), (3,15), (5,24)], columns=['Fib(1,1)', 'Fib(6,9)'])\n",
    "dfc = pd.DataFrame({'Employee-ID': [1001, 1002, 1005], 'Names': ['Alice', 'Bob', 'Clarke'], \n",
    "                    'Income': [100000, 70000, 83000]})\n",
    "dfd = dfc.set_index('Employee-ID')\n",
    "\n",
    "#dfd\n",
    "\n",
    "# We can use the column headers as attributes\n",
    "#dfd.Income\n",
    "type(dfd.Income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have got a rough idea of what a DataFrame is, and how it might be useful, let's see what we can make use of that for analysing time series. We start by reading the rain time series from Bulken into pandas. pandas comes with it's own function to read text files, and one that is more versatile than the functions we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\n",
    "    'd1s2/rr24_Bulken.txt', encoding='utf8', \n",
    "    header=13,\n",
    "    skipfooter=12, engine='python',  # The file contains 12 footer lines, and skipfooter requires the python parsing engine\n",
    "    sep='\\s+',                       # Column separtion by one or more whitespace\n",
    "    parse_dates=[1,], dayfirst=True, # Second column contains dates, in European format\n",
    "    index_col=1,                     # Use the date column as index\n",
    ")\n",
    "\n",
    "\n",
    "#df.RR.plot()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apparently directly plot ``df.RR``, but what else can we do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(df.RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many things available! We'll dive into a few of them later. \n",
    "\n",
    "First, a quick and easy first data analysis: Cumulative precipitation during that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.RR.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: ``cumsum()`` returns a (time) series, so we can work with that in exactly the same way as ``df.RR``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.RR.cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second example, find dates where the 24-hour precipitation exceeded 50 mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[df.RR > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth taking some time to figure out in detail what happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.RR > 50), (df.RR > 50).dtype, len(df.RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a boolean time series to select dates.\n",
    "\n",
    "We could also use any other (random) boolean time series of length 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Note that we use the same index (the dates) as in df.RR\n",
    "randombools = pd.Series(np.random.rand(100) >= 0.9, index=df.RR)\n",
    "randombools.sum() # How many True values (=ones) are there in the time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[randombools]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same mechanism, you can retrieve the entire data row instead of the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.RR > 50]\n",
    "# Or just\n",
    "#df[df.RR > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, you can still select rows by their integer index (starting the count at zero!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 46th and 59th entry in the DataFrame\n",
    "df.iloc[[45,58]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises 1: Monthly cumulative precipitation\n",
    "\n",
    "The rain time series from Bulken contains three full months, August, September and October 2018.\n",
    "\n",
    "Exercises: \n",
    " * Print the cumulative precipitation for these three months to find out which month was wettest.\n",
    " * Plot the cumulative precipitation for these three months into the same figure. \n",
    "\n",
    "Hints: \n",
    " * You will need the ``datetime`` objects from the ``datetime`` package to compare against the time series index.\n",
    " * You might need to convert the cumulative rain time series to a ``numpy`` array for the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "month_names = {8: 'August', 9: 'September', 10: 'October'}\n",
    "for month in [8, 9, 10]:\n",
    "    #df_subset = df[df.index >= datetime(2018,month,1)]\n",
    "    #df_subset = df_subset[df_subset.index < datetime(2018,month+1,1)]\n",
    "    # alternative using logical_and\n",
    "    df_subset = df[np.logical_and(df.index >= datetime(2018,month,1), df.index < datetime(2018,month+1,1))]\n",
    "    print(f'Cumulative precipitation for {month_names[month]}: {df_subset.cumsum().RR.iloc[-1]:3.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for month in [8, 9, 10]:\n",
    "    df_subset = df[np.logical_and(df.index >= datetime(2018,month,1), df.index < datetime(2018,month+1,1))]\n",
    "    RR_per_month = np.array(df_subset.cumsum().RR)\n",
    "    plt.plot(np.arange(len(RR_per_month)), RR_per_month, label=month_names[month])\n",
    "plt.xlabel('Day of the month')\n",
    "plt.ylabel('Cumulative precip. [mm]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing time series\n",
    "\n",
    "So far we have only worked with one time series. Let's add a second to have some more analysis options to explore.\n",
    "\n",
    "Unfortunately, the date format of ``rro_Bulken.txt`` is not recognised automatically by pandas, so we need to supply our custom conversion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_parser = lambda datestr: datetime.strptime(datestr, '%d%m%Y')\n",
    "\n",
    "# Equivalent to\n",
    "def date_parser(datestr):\n",
    "    return datetime.strptime(datestr, '%d%m%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``lambda`` is essentially just a shorthand for defining a function. We can use ``date_parser`` just as any other function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_parser('23012019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_table('d1s2/rro_Bulken.txt', encoding='latin1', \n",
    "                   header=None, names=['Dato', 'Level', 'Discharge', 'p75', 'p50', 'p25'],\n",
    "                                                    # Custom header information\n",
    "                   comment='#',                     # Ignore lines starting with #\n",
    "                   na_values=['----', ],            # Custom marker for missing values\n",
    "                   sep='\\s+',                       # Column separtion by one or more whitespace\n",
    "                   parse_dates=[0,], date_parser=date_parser, \n",
    "                                                    # First column contains dates, custom format\n",
    "                   index_col=0,                     # Use the date column as index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check to see whether we got what we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Discharge.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analyses\n",
    "\n",
    "Is river runoff correlated to precipitation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the different time periods for df and df2!\n",
    "df.RR.corr(df2.Discharge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like!\n",
    "\n",
    "But what about the median discharge and precipitation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.p50.corr(df.RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advance statistics will require the ``scipy.stats`` module. But to be able to use that module, we'll need to homogenise the two time series:\n",
    " * Identical index, i.e. dates\n",
    " * Remove NaNs\n",
    " \n",
    "Once complete, we'll then make use of ``scipy.stats`` to estimate the significance of the above correlations and calculate lagged correlations between the two time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First combine into common data frame, using the same index\n",
    "\n",
    "df_both = pd.concat([df, df2], axis=1)\n",
    "df_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then extract the columns we are interested in, keeping only those rows where we have data in both\n",
    "\n",
    "df_clean = df_both[['RR', 'Discharge']].dropna()\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're finally ready to calculate the Pearson correlaton including it's significance. Documentation for the function is given here:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#scipy.stats.pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "scipy.stats.pearsonr(df_clean.RR, df_clean.Discharge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, seems highly significant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_both[['RR', 'p50']].dropna()\n",
    "scipy.stats.pearsonr(df_clean.RR, df_clean.p50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the correlation between median discharge and precipitation seems spurious.\n",
    "\n",
    "With that clarified, onto lagged correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Lagged correlations\n",
    "\n",
    "We would expect that the river discharge is a delayed response of the precipitation. So we might expect that the maximum correlation is not instantaneous, but at a certain lead/lag.\n",
    "\n",
    "Exercise: At which lead/lag in days does the correlation between precipitation reach it's maximum?\n",
    " * First, calculate the time lagged correlations and their significance for lags between -5 and 5 days. The ``pandas.Series.shift`` function might come in handy to do the actual shifting.\n",
    " * Second, visualise the results with a correlation lead/lag time series. Mark portions of the time series that are significant at the 99%-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First do the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using numpy arrays here in order to be able to filter the data by p-value later\n",
    "tshifts = np.arange(-5,6)\n",
    "corrcoefs = np.empty(tshifts.shape)\n",
    "pvalues = np.empty(tshifts.shape)\n",
    "for i, tshift in zip(range(len(tshifts)), tshifts):\n",
    "    RRshift = df.RR.shift(tshift)\n",
    "    dfs = pd.concat([RRshift, df2.Discharge], axis=1)\n",
    "    dfs = dfs.dropna()\n",
    "    corrcoefs[i], pvalues[i] = scipy.stats.pearsonr(dfs.RR, dfs.Discharge)\n",
    "\n",
    "print(corrcoefs)\n",
    "print(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tshifts, corrcoefs, linewidth=1, color='C0')\n",
    "plt.plot(tshifts[pvalues < 0.01], corrcoefs[pvalues < 0.01], linewidth=3, color='C0')\n",
    "plt.xlabel('<- Discharge leads / Rain leads -> [days]')\n",
    "plt.ylabel('Correlation coefficient')\n",
    "\n",
    "plt.savefig('d2s1/ex2.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Smoothing time series\n",
    "\n",
    "The river discharge might contain an integral of the precipitation over the preceeding days. Let's correlate smoothed precipitation with discharge, to see whether we can further increase the correlations. Use running means of 1-5 days centred on the given date in combination with lags between -5 and 5 days to find the maximum correlation.\n",
    "\n",
    "The ``pandas.Series.rolling`` function might come in handy for calculating the running mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, first the analysis ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmeans = np.arange(1,6)\n",
    "tshifts = np.arange(-5,6)\n",
    "corrcoefs = np.empty(rmeans.shape+tshifts.shape)\n",
    "pvalues = np.empty(rmeans.shape+tshifts.shape)\n",
    "for j, rmean in zip(range(len(rmeans)), rmeans):\n",
    "    RRmean = df.RR.rolling(window=rmean, center=True).mean()\n",
    "    for i, tshift in zip(range(len(tshifts)), tshifts):\n",
    "        RRshift = RRmean.shift(tshift)\n",
    "        dfs = pd.concat([RRshift, df2.Discharge], axis=1)\n",
    "        dfs = dfs.dropna()\n",
    "        corrcoefs[j,i], pvalues[j,i] = scipy.stats.pearsonr(dfs.RR, dfs.Discharge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, rmean in zip(range(len(rmeans)), rmeans):\n",
    "    plt.plot(tshifts, corrcoefs[j], linewidth=1, color='C%d' % j)\n",
    "    plt.plot(tshifts[pvalues[j,:] < 0.01], corrcoefs[j,pvalues[j,:] < 0.01], \n",
    "             linewidth=3, color='C%d' % j, label='%d-day mean' % rmean)\n",
    "plt.xlabel('<- Discharge leads / Rain leads -> [days]')\n",
    "plt.ylabel('Correlation coefficient')\n",
    "plt.legend()\n",
    "plt.savefig('d2s1/ex3.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Fitting linear model\n",
    "\n",
    "For the combination of running mean and time lag that yields the maximum correlation create a linear model to estimate discharge from observed precipitation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first do some preparatory work, before diving into the actual exercise. \n",
    "\n",
    "First step is to extract the time lag / smoothing that yielded the maximum correlation. \n",
    " * The ``np.argmax`` yields the position of the maximum of the flattened the array. A flattened array is one-dimensional and contains the entries of the array in the order in which they appear in memory.\n",
    " * The ``np.unravel_index``-function then converts this index of the flattened array back to a multi-dimensional index appropriate for the original array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the following, make it NOT part of the assignment.\n",
    "jmax, imax = np.unravel_index(np.argmax(corrcoefs), corrcoefs.shape)\n",
    "print('Maximum correlation of %3.1f%% at rmean %d days and lag %d days' % (corrcoefs.max()*100, rmeans[jmax], tshifts[imax]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step is to recreate the shifted running mean that gave the maximum correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RRmax = df.RR.rolling(window=rmeans[jmax], center=True).mean().shift(tshifts[imax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preparation for the linear regression, we then collect all relevant ``Series`` in a new data frame, and rename the shifted / smoothed ``RR`` time series in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column to avoid having two columns named \"RR\", then concatenate to one dataframe\n",
    "RRmax.rename('RRmax', inplace=True)\n",
    "dfs = pd.concat([RRmax, df.RR, df2.Discharge], axis=1)\n",
    "dfs_clean = dfs.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready for the actual exercise\n",
    " * Fit a linear model by linear regression. You can use ``scipy.stats.linregress`` do to the actual regression.\n",
    " * Create a new time series containing the river discharge from the precipitation observations\n",
    " * Evaluate the linear model fit using a ``plt.scatter``-plot comparing modelled versus observed discharge.\n",
    " * Evaluete the linear model by comparing the modelled and observed discharge time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps 1+2: Fitting the linear model and create the modelled discharge time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linmodel = scipy.stats.linregress(dfs_clean.RRmax, dfs_clean.Discharge)\n",
    "estimate_Discharge = lambda RRm: linmodel.slope*RRm + linmodel.intercept\n",
    "dfs['Discharge_linmodel'] = pd.Series(estimate_Discharge(dfs.RRmax), index=dfs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Evaluation using scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dfs.Discharge, dfs.Discharge_linmodel)\n",
    "plt.plot([0, 500], [0, 500], 'k-', linewidth=1)\n",
    "plt.xlabel('Observed discharge [m3/s]')\n",
    "plt.ylabel('Estimated discharge from precipitation [m3/s]')\n",
    "plt.savefig('d2s1/ex3_scatter.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Evaluation comparing the time series directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_clean = dfs.dropna()\n",
    "dfs_clean.Discharge.plot(label='Observed')\n",
    "dfs_clean.Discharge_linmodel.plot(label='Estimated')\n",
    "plt.ylabel('Discharge [m3/s]')\n",
    "plt.legend()\n",
    "plt.savefig('d2s1/ex3_timeseries.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "83e4dae74d7a452da845a83c66caccd5",
   "lastKernelId": "64f512d6-5e02-4551-b6f0-243f0c156747"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
